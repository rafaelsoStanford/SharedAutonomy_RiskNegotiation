{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1lA25xjBqlSfoW1qgBMcuElpeC8Mfmy2r",
      "authorship_tag": "ABX9TyNEoLA52vOxzwpKT9NlRRko",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1d69b5d787a3484290407ce68ac7b765": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a28568446d954460bd461496131e6b2d",
              "IPY_MODEL_b6f53460b7594f2488a30beecadab190",
              "IPY_MODEL_4745ce986c2941d8b2fd6d543211ddc2"
            ],
            "layout": "IPY_MODEL_3592ad22550f4bb8ab78f6c4935475c8"
          }
        },
        "a28568446d954460bd461496131e6b2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f729f98087c74e1e94b9f6ac1d8629d6",
            "placeholder": "​",
            "style": "IPY_MODEL_069b4d1bd9c84f62b4b0d111ae4374ff",
            "value": "Epoch:   0%"
          }
        },
        "b6f53460b7594f2488a30beecadab190": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77dcbbb0c73b4647a064720d38659eb0",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_92f8d292b0d84062bea3eea0e64e3b27",
            "value": 0
          }
        },
        "4745ce986c2941d8b2fd6d543211ddc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c3e11ab000f47cc9c2bd4336b20b1fc",
            "placeholder": "​",
            "style": "IPY_MODEL_d4216db049b144689959de94ab022844",
            "value": " 0/100 [00:06&lt;?, ?it/s]"
          }
        },
        "3592ad22550f4bb8ab78f6c4935475c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f729f98087c74e1e94b9f6ac1d8629d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "069b4d1bd9c84f62b4b0d111ae4374ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77dcbbb0c73b4647a064720d38659eb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92f8d292b0d84062bea3eea0e64e3b27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c3e11ab000f47cc9c2bd4336b20b1fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4216db049b144689959de94ab022844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d210f1c104c54f9094e23ab5d601447c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7999fb8194d64130a1c9ac5249daa9af",
              "IPY_MODEL_120b19d92c6e41c39824822f00887f89",
              "IPY_MODEL_ac13ad050f9f499cbeaa01f0d12a5da3"
            ],
            "layout": "IPY_MODEL_02172c34e9fc46cfa13834c948aed9b0"
          }
        },
        "7999fb8194d64130a1c9ac5249daa9af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2cbef52354248f287a150b057821605",
            "placeholder": "​",
            "style": "IPY_MODEL_aa2a16bf98514a1bb4e7b49a1b91ba7e",
            "value": "Batch:   0%"
          }
        },
        "120b19d92c6e41c39824822f00887f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47571b864cd24dae94d31f4879dbdbc5",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_347093edb5ff4c0a8338d1fa65320e64",
            "value": 0
          }
        },
        "ac13ad050f9f499cbeaa01f0d12a5da3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24cb2a612c8642839467f6da05e28ae9",
            "placeholder": "​",
            "style": "IPY_MODEL_da187735d7f942e4b53f724b55964638",
            "value": " 0/8 [00:06&lt;?, ?it/s]"
          }
        },
        "02172c34e9fc46cfa13834c948aed9b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2cbef52354248f287a150b057821605": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa2a16bf98514a1bb4e7b49a1b91ba7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47571b864cd24dae94d31f4879dbdbc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "347093edb5ff4c0a8338d1fa65320e64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24cb2a612c8642839467f6da05e28ae9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da187735d7f942e4b53f724b55964638": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafaelsoStanford/SharedAutonomy_RiskNegotiation/blob/main/CarRacingDiffusion_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Tf3JP6vSyP4",
        "outputId": "1a4fdbd6-5648-4717-d0c5-d73a0818e778",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.11\n"
          ]
        }
      ],
      "source": [
        "#@markdown ### **Installing pip packages**\n",
        "#@markdown - Diffusion Model: [PyTorch](https://pytorch.org) & [HuggingFace diffusers](https://huggingface.co/docs/diffusers/index)\n",
        "#@markdown - Dataset Loading: [Zarr](https://zarr.readthedocs.io/en/stable/) & numcodecs\n",
        "#@markdown -  gym, pygame, pymunk & shapely\n",
        "!python --version\n",
        "!apt install swig &> /dev/null\n",
        "!pip3 uninstall cvxpy -y > /dev/null\n",
        "!pip3 install gymnasium[box2d] &> /dev/null\n",
        "!pip3 install torch==1.13.1 torchvision==0.14.1 diffusers==0.11.1 \\\n",
        "scikit-image==0.19.3 scikit-video==1.1.11 zarr==2.12.0 numcodecs==0.10.2 \\\n",
        "&> /dev/null # mute output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Imports**\n",
        "# diffusion policy import\n",
        "from typing import Tuple, Sequence, Dict, Union, Optional, Callable\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import collections\n",
        "import zarr\n",
        "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
        "from diffusers.training_utils import EMAModel\n",
        "from diffusers.optimization import get_scheduler\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# env import\n",
        "\n",
        "import pygame\n",
        "from pygame import gfxdraw\n",
        "\n",
        "import shapely.geometry as sg\n",
        "import cv2\n",
        "import skimage.transform as st\n",
        "from skvideo.io import vwrite\n",
        "from IPython.display import Video\n",
        "import gdown\n",
        "import os\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from gymnasium.envs.box2d.car_dynamics import Car\n",
        "from gymnasium.error import DependencyNotInstalled, InvalidAction\n",
        "from gymnasium.utils import EzPickle\n",
        "from typing import Optional, Union\n",
        "\n",
        "import Box2D\n",
        "from Box2D.b2 import fixtureDef\n",
        "from Box2D.b2 import polygonShape\n",
        "from Box2D.b2 import contactListener\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yflI2ihsS81P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d595ae8-6941-472d-f6bd-d82e73427c4a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame 2.1.3 (SDL 2.0.22, Python 3.10.11)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Environment**\n",
        "#@markdown Slightly modified version of the CarRacing Environment \"CarRacing-v2\".\n",
        "#@markdown Adapted from [Gymnasium](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/box2d/car_racing.py)\n",
        "\n",
        "\n",
        "\n",
        "__credits__ = [\"Andrea PIERRÉ\"]\n",
        "\n",
        "\n",
        "STATE_W = 96  # less than Atari 160x192\n",
        "STATE_H = 96\n",
        "VIDEO_W = 600\n",
        "VIDEO_H = 400\n",
        "WINDOW_W = 1000\n",
        "WINDOW_H = 800\n",
        "\n",
        "SCALE = 6.0  # Track scale\n",
        "TRACK_RAD = 900 / SCALE  # Track is heavily morphed circle with this radius\n",
        "PLAYFIELD = 2000 / SCALE  # Game over boundary\n",
        "FPS = 50  # Frames per second\n",
        "ZOOM = 2.7  # Camera zoom\n",
        "ZOOM_FOLLOW = True  # Set to False for fixed view (don't use zoom)\n",
        "\n",
        "\n",
        "TRACK_DETAIL_STEP = 21 / SCALE\n",
        "TRACK_TURN_RATE = 0.31\n",
        "TRACK_WIDTH = 40 / SCALE\n",
        "BORDER = 8 / SCALE\n",
        "BORDER_MIN_COUNT = 4\n",
        "GRASS_DIM = PLAYFIELD / 20.0\n",
        "MAX_SHAPE_DIM = (\n",
        "    max(GRASS_DIM, TRACK_WIDTH, TRACK_DETAIL_STEP) * math.sqrt(2) * ZOOM * SCALE\n",
        ")\n",
        "\n",
        "\n",
        "class FrictionDetector(contactListener):\n",
        "    def __init__(self, env, lap_complete_percent):\n",
        "        contactListener.__init__(self)\n",
        "        self.env = env\n",
        "        self.lap_complete_percent = lap_complete_percent\n",
        "\n",
        "    def BeginContact(self, contact):\n",
        "        self._contact(contact, True)\n",
        "\n",
        "    def EndContact(self, contact):\n",
        "        self._contact(contact, False)\n",
        "\n",
        "    def _contact(self, contact, begin):\n",
        "        tile = None\n",
        "        obj = None\n",
        "        u1 = contact.fixtureA.body.userData\n",
        "        u2 = contact.fixtureB.body.userData\n",
        "        if u1 and \"road_friction\" in u1.__dict__:\n",
        "            tile = u1\n",
        "            obj = u2\n",
        "        if u2 and \"road_friction\" in u2.__dict__:\n",
        "            tile = u2\n",
        "            obj = u1\n",
        "        if not tile:\n",
        "            return\n",
        "\n",
        "        # inherit tile color from env\n",
        "        tile.color[:] = self.env.road_color\n",
        "        if not obj or \"tiles\" not in obj.__dict__:\n",
        "            return\n",
        "        if begin:\n",
        "            obj.tiles.add(tile)\n",
        "            if not tile.road_visited:\n",
        "                tile.road_visited = True\n",
        "                self.env.reward += 1000.0 / len(self.env.track)\n",
        "                self.env.tile_visited_count += 1\n",
        "\n",
        "                # Lap is considered completed if enough % of the track was covered\n",
        "                if (\n",
        "                    tile.idx == 0\n",
        "                    and self.env.tile_visited_count / len(self.env.track)\n",
        "                    > self.lap_complete_percent\n",
        "                ):\n",
        "                    self.env.new_lap = True\n",
        "        else:\n",
        "            obj.tiles.remove(tile)\n",
        "\n",
        "\n",
        "class CarRacing(gym.Env, EzPickle):\n",
        "    \"\"\"\n",
        "    ## Description\n",
        "    The easiest control task to learn from pixels - a top-down\n",
        "    racing environment. The generated track is random every episode.\n",
        "\n",
        "    Some indicators are shown at the bottom of the window along with the\n",
        "    state RGB buffer. From left to right: true speed, four ABS sensors,\n",
        "    steering wheel position, and gyroscope.\n",
        "    To play yourself (it's rather fast for humans), type:\n",
        "    ```\n",
        "    python gymnasium/envs/box2d/car_racing.py\n",
        "    ```\n",
        "    Remember: it's a powerful rear-wheel drive car - don't press the accelerator\n",
        "    and turn at the same time.\n",
        "\n",
        "    ## Action Space\n",
        "    If continuous there are 3 actions :\n",
        "    - 0: steering, -1 is full left, +1 is full right\n",
        "    - 1: gas\n",
        "    - 2: breaking\n",
        "\n",
        "    If discrete there are 5 actions:\n",
        "    - 0: do nothing\n",
        "    - 1: steer left\n",
        "    - 2: steer right\n",
        "    - 3: gas\n",
        "    - 4: brake\n",
        "\n",
        "    ## Observation Space\n",
        "\n",
        "    A top-down 96x96 RGB image of the car and race track.\n",
        "\n",
        "    ## Rewards\n",
        "    The reward is -0.1 every frame and +1000/N for every track tile visited,\n",
        "    where N is the total number of tiles visited in the track. For example,\n",
        "    if you have finished in 732 frames, your reward is\n",
        "    1000 - 0.1*732 = 926.8 points.\n",
        "\n",
        "    ## Starting State\n",
        "    The car starts at rest in the center of the road.\n",
        "\n",
        "    ## Episode Termination\n",
        "    The episode finishes when all the tiles are visited. The car can also go\n",
        "    outside the playfield - that is, far off the track, in which case it will\n",
        "    receive -100 reward and die.\n",
        "\n",
        "    ## Arguments\n",
        "    `lap_complete_percent` dictates the percentage of tiles that must be visited by\n",
        "    the agent before a lap is considered complete.\n",
        "\n",
        "    Passing `domain_randomize=True` enables the domain randomized variant of the environment.\n",
        "    In this scenario, the background and track colours are different on every reset.\n",
        "\n",
        "    Passing `continuous=False` converts the environment to use discrete action space.\n",
        "    The discrete action space has 5 actions: [do nothing, left, right, gas, brake].\n",
        "\n",
        "    ## Reset Arguments\n",
        "    Passing the option `options[\"randomize\"] = True` will change the current colour of the environment on demand.\n",
        "    Correspondingly, passing the option `options[\"randomize\"] = False` will not change the current colour of the environment.\n",
        "    `domain_randomize` must be `True` on init for this argument to work.\n",
        "    Example usage:\n",
        "    ```python\n",
        "    import gymnasium as gym\n",
        "    env = gym.make(\"CarRacing-v1\", domain_randomize=True)\n",
        "\n",
        "    # normal reset, this changes the colour scheme by default\n",
        "    env.reset()\n",
        "\n",
        "    # reset with colour scheme change\n",
        "    env.reset(options={\"randomize\": True})\n",
        "\n",
        "    # reset with no colour scheme change\n",
        "    env.reset(options={\"randomize\": False})\n",
        "    ```\n",
        "\n",
        "    ## Version History\n",
        "    - v1: Change track completion logic and add domain randomization (0.24.0)\n",
        "    - v0: Original version\n",
        "\n",
        "    ## References\n",
        "    - Chris Campbell (2014), http://www.iforce2d.net/b2dtut/top-down-car.\n",
        "\n",
        "    ## Credits\n",
        "    Created by Oleg Klimov\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\n",
        "        \"render_modes\": [\n",
        "            \"human\",\n",
        "            \"rgb_array\",\n",
        "            \"state_pixels\",\n",
        "        ],\n",
        "        \"render_fps\": FPS,\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        render_mode: Optional[str] = None,\n",
        "        verbose: bool = False,\n",
        "        lap_complete_percent: float = 0.95,\n",
        "        domain_randomize: bool = False,\n",
        "        continuous: bool = True,\n",
        "    ):\n",
        "        EzPickle.__init__(\n",
        "            self,\n",
        "            render_mode,\n",
        "            verbose,\n",
        "            lap_complete_percent,\n",
        "            domain_randomize,\n",
        "            continuous,\n",
        "        )\n",
        "        self.continuous = continuous\n",
        "        self.domain_randomize = domain_randomize\n",
        "        self.lap_complete_percent = lap_complete_percent\n",
        "        self._init_colors()\n",
        "\n",
        "        self.contactListener_keepref = FrictionDetector(self, self.lap_complete_percent)\n",
        "        self.world = Box2D.b2World((0, 0), contactListener=self.contactListener_keepref)\n",
        "        self.screen: Optional[pygame.Surface] = None\n",
        "        self.surf = None\n",
        "        self.clock = None\n",
        "        self.isopen = True\n",
        "        self.invisible_state_window = None\n",
        "        self.invisible_video_window = None\n",
        "        self.road = None\n",
        "        self.car: Optional[Car] = None\n",
        "        self.reward = 0.0\n",
        "        self.prev_reward = 0.0\n",
        "        self.verbose = verbose\n",
        "        self.new_lap = False\n",
        "        self.fd_tile = fixtureDef(\n",
        "            shape=polygonShape(vertices=[(0, 0), (1, 0), (1, -1), (0, -1)])\n",
        "        )\n",
        "\n",
        "        # This will throw a warning in tests/envs/test_envs in utils/env_checker.py as the space is not symmetric\n",
        "        #   or normalised however this is not possible here so ignore\n",
        "        if self.continuous:\n",
        "            self.action_space = spaces.Box(\n",
        "                np.array([-1, 0, 0]).astype(np.float32),\n",
        "                np.array([+1, +1, +1]).astype(np.float32),\n",
        "            )  # steer, gas, brake\n",
        "        else:\n",
        "            self.action_space = spaces.Discrete(5)\n",
        "            # do nothing, left, right, gas, brake\n",
        "\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=255, shape=(STATE_H, STATE_W, 3), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "    def _destroy(self):\n",
        "        if not self.road:\n",
        "            return\n",
        "        for t in self.road:\n",
        "            self.world.DestroyBody(t)\n",
        "        self.road = []\n",
        "        assert self.car is not None\n",
        "        self.car.destroy()\n",
        "\n",
        "    def _init_colors(self):\n",
        "        if self.domain_randomize:\n",
        "            # domain randomize the bg and grass colour\n",
        "            self.road_color = self.np_random.uniform(0, 210, size=3)\n",
        "\n",
        "            self.bg_color = self.np_random.uniform(0, 210, size=3)\n",
        "\n",
        "            self.grass_color = np.copy(self.bg_color)\n",
        "            idx = self.np_random.integers(3)\n",
        "            self.grass_color[idx] += 20\n",
        "        else:\n",
        "            # default colours\n",
        "            self.road_color = np.array([102, 102, 102])\n",
        "            self.bg_color = np.array([102, 204, 102])\n",
        "            self.grass_color = np.array([102, 230, 102])\n",
        "\n",
        "    def _reinit_colors(self, randomize):\n",
        "        assert (\n",
        "            self.domain_randomize\n",
        "        ), \"domain_randomize must be True to use this function.\"\n",
        "\n",
        "        if randomize:\n",
        "            # domain randomize the bg and grass colour\n",
        "            self.road_color = self.np_random.uniform(0, 210, size=3)\n",
        "\n",
        "            self.bg_color = self.np_random.uniform(0, 210, size=3)\n",
        "\n",
        "            self.grass_color = np.copy(self.bg_color)\n",
        "            idx = self.np_random.integers(3)\n",
        "            self.grass_color[idx] += 20\n",
        "\n",
        "    def _create_track(self):\n",
        "        CHECKPOINTS = 12\n",
        "\n",
        "        # Create checkpoints\n",
        "        checkpoints = []\n",
        "        for c in range(CHECKPOINTS):\n",
        "            noise = self.np_random.uniform(0, 2 * math.pi * 1 / CHECKPOINTS)\n",
        "            alpha = 2 * math.pi * c / CHECKPOINTS + noise\n",
        "            rad = self.np_random.uniform(TRACK_RAD / 3, TRACK_RAD)\n",
        "\n",
        "            if c == 0:\n",
        "                alpha = 0\n",
        "                rad = 1.5 * TRACK_RAD\n",
        "            if c == CHECKPOINTS - 1:\n",
        "                alpha = 2 * math.pi * c / CHECKPOINTS\n",
        "                self.start_alpha = 2 * math.pi * (-0.5) / CHECKPOINTS\n",
        "                rad = 1.5 * TRACK_RAD\n",
        "\n",
        "            checkpoints.append((alpha, rad * math.cos(alpha), rad * math.sin(alpha)))\n",
        "        self.road = []\n",
        "\n",
        "        # Go from one checkpoint to another to create track\n",
        "        x, y, beta = 1.5 * TRACK_RAD, 0, 0\n",
        "        dest_i = 0\n",
        "        laps = 0\n",
        "        track = []\n",
        "        no_freeze = 2500\n",
        "        visited_other_side = False\n",
        "        while True:\n",
        "            alpha = math.atan2(y, x)\n",
        "            if visited_other_side and alpha > 0:\n",
        "                laps += 1\n",
        "                visited_other_side = False\n",
        "            if alpha < 0:\n",
        "                visited_other_side = True\n",
        "                alpha += 2 * math.pi\n",
        "\n",
        "            while True:  # Find destination from checkpoints\n",
        "                failed = True\n",
        "\n",
        "                while True:\n",
        "                    dest_alpha, dest_x, dest_y = checkpoints[dest_i % len(checkpoints)]\n",
        "                    if alpha <= dest_alpha:\n",
        "                        failed = False\n",
        "                        break\n",
        "                    dest_i += 1\n",
        "                    if dest_i % len(checkpoints) == 0:\n",
        "                        break\n",
        "\n",
        "                if not failed:\n",
        "                    break\n",
        "\n",
        "                alpha -= 2 * math.pi\n",
        "                continue\n",
        "\n",
        "            r1x = math.cos(beta)\n",
        "            r1y = math.sin(beta)\n",
        "            p1x = -r1y\n",
        "            p1y = r1x\n",
        "            dest_dx = dest_x - x  # vector towards destination\n",
        "            dest_dy = dest_y - y\n",
        "            # destination vector projected on rad:\n",
        "            proj = r1x * dest_dx + r1y * dest_dy\n",
        "            while beta - alpha > 1.5 * math.pi:\n",
        "                beta -= 2 * math.pi\n",
        "            while beta - alpha < -1.5 * math.pi:\n",
        "                beta += 2 * math.pi\n",
        "            prev_beta = beta\n",
        "            proj *= SCALE\n",
        "            if proj > 0.3:\n",
        "                beta -= min(TRACK_TURN_RATE, abs(0.001 * proj))\n",
        "            if proj < -0.3:\n",
        "                beta += min(TRACK_TURN_RATE, abs(0.001 * proj))\n",
        "            x += p1x * TRACK_DETAIL_STEP\n",
        "            y += p1y * TRACK_DETAIL_STEP\n",
        "            track.append((alpha, prev_beta * 0.5 + beta * 0.5, x, y))\n",
        "            if laps > 4:\n",
        "                break\n",
        "            no_freeze -= 1\n",
        "            if no_freeze == 0:\n",
        "                break\n",
        "\n",
        "        # Find closed loop range i1..i2, first loop should be ignored, second is OK\n",
        "        i1, i2 = -1, -1\n",
        "        i = len(track)\n",
        "        while True:\n",
        "            i -= 1\n",
        "            if i == 0:\n",
        "                return False  # Failed\n",
        "            pass_through_start = (\n",
        "                track[i][0] > self.start_alpha and track[i - 1][0] <= self.start_alpha\n",
        "            )\n",
        "            if pass_through_start and i2 == -1:\n",
        "                i2 = i\n",
        "            elif pass_through_start and i1 == -1:\n",
        "                i1 = i\n",
        "                break\n",
        "        if self.verbose:\n",
        "            print(\"Track generation: %i..%i -> %i-tiles track\" % (i1, i2, i2 - i1))\n",
        "        assert i1 != -1\n",
        "        assert i2 != -1\n",
        "\n",
        "        track = track[i1 : i2 - 1]\n",
        "\n",
        "        first_beta = track[0][1]\n",
        "        first_perp_x = math.cos(first_beta)\n",
        "        first_perp_y = math.sin(first_beta)\n",
        "        # Length of perpendicular jump to put together head and tail\n",
        "        well_glued_together = np.sqrt(\n",
        "            np.square(first_perp_x * (track[0][2] - track[-1][2]))\n",
        "            + np.square(first_perp_y * (track[0][3] - track[-1][3]))\n",
        "        )\n",
        "        if well_glued_together > TRACK_DETAIL_STEP:\n",
        "            return False\n",
        "\n",
        "        # Red-white border on hard turns\n",
        "        border = [False] * len(track)\n",
        "        for i in range(len(track)):\n",
        "            good = True\n",
        "            oneside = 0\n",
        "            for neg in range(BORDER_MIN_COUNT):\n",
        "                beta1 = track[i - neg - 0][1]\n",
        "                beta2 = track[i - neg - 1][1]\n",
        "                good &= abs(beta1 - beta2) > TRACK_TURN_RATE * 0.2\n",
        "                oneside += np.sign(beta1 - beta2)\n",
        "            good &= abs(oneside) == BORDER_MIN_COUNT\n",
        "            border[i] = good\n",
        "        for i in range(len(track)):\n",
        "            for neg in range(BORDER_MIN_COUNT):\n",
        "                border[i - neg] |= border[i]\n",
        "\n",
        "        # Create tiles\n",
        "        for i in range(len(track)):\n",
        "            alpha1, beta1, x1, y1 = track[i]\n",
        "            alpha2, beta2, x2, y2 = track[i - 1]\n",
        "            road1_l = (\n",
        "                x1 - TRACK_WIDTH * math.cos(beta1),\n",
        "                y1 - TRACK_WIDTH * math.sin(beta1),\n",
        "            )\n",
        "            road1_r = (\n",
        "                x1 + TRACK_WIDTH * math.cos(beta1),\n",
        "                y1 + TRACK_WIDTH * math.sin(beta1),\n",
        "            )\n",
        "            road2_l = (\n",
        "                x2 - TRACK_WIDTH * math.cos(beta2),\n",
        "                y2 - TRACK_WIDTH * math.sin(beta2),\n",
        "            )\n",
        "            road2_r = (\n",
        "                x2 + TRACK_WIDTH * math.cos(beta2),\n",
        "                y2 + TRACK_WIDTH * math.sin(beta2),\n",
        "            )\n",
        "            vertices = [road1_l, road1_r, road2_r, road2_l]\n",
        "            self.fd_tile.shape.vertices = vertices\n",
        "            t = self.world.CreateStaticBody(fixtures=self.fd_tile)\n",
        "            t.userData = t\n",
        "            c = 0.01 * (i % 3) * 255\n",
        "            t.color = self.road_color + c\n",
        "            t.road_visited = False\n",
        "            t.road_friction = 1.0\n",
        "            t.idx = i\n",
        "            t.fixtures[0].sensor = True\n",
        "            self.road_poly.append(([road1_l, road1_r, road2_r, road2_l], t.color))\n",
        "            self.road.append(t)\n",
        "            if border[i]:\n",
        "                side = np.sign(beta2 - beta1)\n",
        "                b1_l = (\n",
        "                    x1 + side * TRACK_WIDTH * math.cos(beta1),\n",
        "                    y1 + side * TRACK_WIDTH * math.sin(beta1),\n",
        "                )\n",
        "                b1_r = (\n",
        "                    x1 + side * (TRACK_WIDTH + BORDER) * math.cos(beta1),\n",
        "                    y1 + side * (TRACK_WIDTH + BORDER) * math.sin(beta1),\n",
        "                )\n",
        "                b2_l = (\n",
        "                    x2 + side * TRACK_WIDTH * math.cos(beta2),\n",
        "                    y2 + side * TRACK_WIDTH * math.sin(beta2),\n",
        "                )\n",
        "                b2_r = (\n",
        "                    x2 + side * (TRACK_WIDTH + BORDER) * math.cos(beta2),\n",
        "                    y2 + side * (TRACK_WIDTH + BORDER) * math.sin(beta2),\n",
        "                )\n",
        "                self.road_poly.append(\n",
        "                    (\n",
        "                        [b1_l, b1_r, b2_r, b2_l],\n",
        "                        (255, 255, 255) if i % 2 == 0 else (255, 0, 0),\n",
        "                    )\n",
        "                )\n",
        "        self.track = track\n",
        "        return True\n",
        "\n",
        "    def reset(\n",
        "        self,\n",
        "        *,\n",
        "        seed: Optional[int] = None,\n",
        "        options: Optional[dict] = None,\n",
        "    ):\n",
        "        super().reset(seed=seed)\n",
        "        self._destroy()\n",
        "        self.world.contactListener_bug_workaround = FrictionDetector(\n",
        "            self, self.lap_complete_percent\n",
        "        )\n",
        "        self.world.contactListener = self.world.contactListener_bug_workaround\n",
        "        self.reward = 0.0\n",
        "        self.prev_reward = 0.0\n",
        "        self.tile_visited_count = 0\n",
        "        self.t = 0.0\n",
        "        self.new_lap = False\n",
        "        self.road_poly = []\n",
        "\n",
        "        if self.domain_randomize:\n",
        "            randomize = True\n",
        "            if isinstance(options, dict):\n",
        "                if \"randomize\" in options:\n",
        "                    randomize = options[\"randomize\"]\n",
        "\n",
        "            self._reinit_colors(randomize)\n",
        "\n",
        "        while True:\n",
        "            success = self._create_track()\n",
        "            if success:\n",
        "                break\n",
        "            if self.verbose:\n",
        "                print(\n",
        "                    \"retry to generate track (normal if there are not many\"\n",
        "                    \"instances of this message)\"\n",
        "                )\n",
        "        self.car = Car(self.world, *self.track[0][1:4])\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        return self.step(None)[0], {}\n",
        "\n",
        "    def return_velocity(self):\n",
        "        return self.car.hull.linearVelocity\n",
        "\n",
        "    def return_abs_velocity(self):\n",
        "        v= self.car.hull.linearVelocity\n",
        "        return np.linalg.norm(v)\n",
        "\n",
        "    def step(self, action: Union[np.ndarray, int]):\n",
        "        assert self.car is not None\n",
        "        if action is not None:\n",
        "            if self.continuous:\n",
        "                self.car.steer(-action[0])\n",
        "                self.car.gas(action[1])\n",
        "                self.car.brake(action[2])\n",
        "            else:\n",
        "                if not self.action_space.contains(action):\n",
        "                    raise InvalidAction(\n",
        "                        f\"you passed the invalid action `{action}`. \"\n",
        "                        f\"The supported action_space is `{self.action_space}`\"\n",
        "                    )\n",
        "                self.car.steer(-0.6 * (action == 1) + 0.6 * (action == 2))\n",
        "                self.car.gas(0.2 * (action == 3))\n",
        "                self.car.brake(0.8 * (action == 4))\n",
        "\n",
        "        self.car.step(1.0 / FPS)\n",
        "        self.world.Step(1.0 / FPS, 6 * 30, 2 * 30)\n",
        "        self.t += 1.0 / FPS\n",
        "\n",
        "        self.state = self._render(\"state_pixels\")\n",
        "\n",
        "        step_reward = 0\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        if action is not None:  # First step without action, called from reset()\n",
        "            self.reward -= 0.1\n",
        "            # We actually don't want to count fuel spent, we want car to be faster.\n",
        "            # self.reward -=  10 * self.car.fuel_spent / ENGINE_POWER\n",
        "            self.car.fuel_spent = 0.0\n",
        "            step_reward = self.reward - self.prev_reward\n",
        "            self.prev_reward = self.reward\n",
        "            if self.tile_visited_count == len(self.track) or self.new_lap:\n",
        "                # Truncation due to finishing lap\n",
        "                # This should not be treated as a failure\n",
        "                # but like a timeout\n",
        "                truncated = True\n",
        "            x, y = self.car.hull.position\n",
        "            if abs(x) > PLAYFIELD or abs(y) > PLAYFIELD:\n",
        "                terminated = True\n",
        "                step_reward = -100\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        \n",
        "        # check if car is on track\n",
        "        on_track = False\n",
        "        for wheel in self.car.wheels:\n",
        "            if wheel.tiles != 0:\n",
        "                on_track = True\n",
        "                break\n",
        "\n",
        "        # create observation dict\n",
        "        observation = {\n",
        "            \"image\": self.state,\n",
        "            \"velocity\": self.return_abs_velocity()\n",
        "#            \"on_track\": on_track\n",
        "        }\n",
        "        \n",
        "        #if action is None:\n",
        "        #  return self.state, step_reward, terminated, truncated, {}\n",
        "        return observation, step_reward, terminated, truncated, {}\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode is None:\n",
        "            assert self.spec is not None\n",
        "            gym.logger.warn(\n",
        "                \"You are calling render method without specifying any render mode. \"\n",
        "                \"You can specify the render_mode at initialization, \"\n",
        "                f'e.g. gym.make(\"{self.spec.id}\", render_mode=\"rgb_array\")'\n",
        "            )\n",
        "            return\n",
        "        else:\n",
        "            return self._render(self.render_mode)\n",
        "\n",
        "    def _render(self, mode: str):\n",
        "        assert mode in self.metadata[\"render_modes\"]\n",
        "\n",
        "        pygame.font.init()\n",
        "        if self.screen is None and mode == \"human\":\n",
        "            pygame.init()\n",
        "            pygame.display.init()\n",
        "            self.screen = pygame.display.set_mode((WINDOW_W, WINDOW_H))\n",
        "        if self.clock is None:\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "        if \"t\" not in self.__dict__:\n",
        "            return  # reset() not called yet\n",
        "\n",
        "        self.surf = pygame.Surface((WINDOW_W, WINDOW_H))\n",
        "\n",
        "        assert self.car is not None\n",
        "        # computing transformations\n",
        "        angle = -self.car.hull.angle\n",
        "        # Animating first second zoom.\n",
        "        zoom = 0.1 * SCALE * max(1 - self.t, 0) + ZOOM * SCALE * min(self.t, 1)\n",
        "        scroll_x = -(self.car.hull.position[0]) * zoom\n",
        "        scroll_y = -(self.car.hull.position[1]) * zoom\n",
        "        trans = pygame.math.Vector2((scroll_x, scroll_y)).rotate_rad(angle)\n",
        "        trans = (WINDOW_W / 2 + trans[0], WINDOW_H / 4 + trans[1])\n",
        "\n",
        "        self._render_road(zoom, trans, angle)\n",
        "        self.car.draw(\n",
        "            self.surf,\n",
        "            zoom,\n",
        "            trans,\n",
        "            angle,\n",
        "            mode not in [\"state_pixels_list\", \"state_pixels\"],\n",
        "        )\n",
        "\n",
        "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
        "\n",
        "        # showing stats\n",
        "        self._render_indicators(WINDOW_W, WINDOW_H)\n",
        "\n",
        "        font = pygame.font.Font(pygame.font.get_default_font(), 42)\n",
        "        text = font.render(\"%04i\" % self.reward, True, (255, 255, 255), (0, 0, 0))\n",
        "        text_rect = text.get_rect()\n",
        "        text_rect.center = (60, WINDOW_H - WINDOW_H * 2.5 / 40.0)\n",
        "        self.surf.blit(text, text_rect)\n",
        "\n",
        "        if mode == \"human\":\n",
        "            pygame.event.pump()\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "            assert self.screen is not None\n",
        "            self.screen.fill(0)\n",
        "            self.screen.blit(self.surf, (0, 0))\n",
        "            pygame.display.flip()\n",
        "        elif mode == \"rgb_array\":\n",
        "            return self._create_image_array(self.surf, (VIDEO_W, VIDEO_H))\n",
        "            \n",
        "        elif mode == \"state_pixels\":\n",
        "            return self._create_image_array(self.surf, (STATE_W, STATE_H))\n",
        "        else:\n",
        "            return self.isopen\n",
        "\n",
        "    def _render_road(self, zoom, translation, angle):\n",
        "        bounds = PLAYFIELD\n",
        "        field = [\n",
        "            (bounds, bounds),\n",
        "            (bounds, -bounds),\n",
        "            (-bounds, -bounds),\n",
        "            (-bounds, bounds),\n",
        "        ]\n",
        "\n",
        "        # draw background\n",
        "        self._draw_colored_polygon(\n",
        "            self.surf, field, self.bg_color, zoom, translation, angle, clip=False\n",
        "        )\n",
        "\n",
        "        # draw grass patches\n",
        "        grass = []\n",
        "        for x in range(-20, 20, 2):\n",
        "            for y in range(-20, 20, 2):\n",
        "                grass.append(\n",
        "                    [\n",
        "                        (GRASS_DIM * x + GRASS_DIM, GRASS_DIM * y + 0),\n",
        "                        (GRASS_DIM * x + 0, GRASS_DIM * y + 0),\n",
        "                        (GRASS_DIM * x + 0, GRASS_DIM * y + GRASS_DIM),\n",
        "                        (GRASS_DIM * x + GRASS_DIM, GRASS_DIM * y + GRASS_DIM),\n",
        "                    ]\n",
        "                )\n",
        "        for poly in grass:\n",
        "            self._draw_colored_polygon(\n",
        "                self.surf, poly, self.grass_color, zoom, translation, angle\n",
        "            )\n",
        "\n",
        "        # draw road\n",
        "        for poly, color in self.road_poly:\n",
        "            # converting to pixel coordinates\n",
        "            poly = [(p[0], p[1]) for p in poly]\n",
        "            color = [int(c) for c in color]\n",
        "            self._draw_colored_polygon(self.surf, poly, color, zoom, translation, angle)\n",
        "\n",
        "    def _render_indicators(self, W, H):\n",
        "        s = W / 40.0\n",
        "        h = H / 40.0\n",
        "        color = (0, 0, 0)\n",
        "        polygon = [(W, H), (W, H - 5 * h), (0, H - 5 * h), (0, H)]\n",
        "        pygame.draw.polygon(self.surf, color=color, points=polygon)\n",
        "\n",
        "        def vertical_ind(place, val):\n",
        "            return [\n",
        "                (place * s, H - (h + h * val)),\n",
        "                ((place + 1) * s, H - (h + h * val)),\n",
        "                ((place + 1) * s, H - h),\n",
        "                ((place + 0) * s, H - h),\n",
        "            ]\n",
        "\n",
        "        def horiz_ind(place, val):\n",
        "            return [\n",
        "                ((place + 0) * s, H - 4 * h),\n",
        "                ((place + val) * s, H - 4 * h),\n",
        "                ((place + val) * s, H - 2 * h),\n",
        "                ((place + 0) * s, H - 2 * h),\n",
        "            ]\n",
        "\n",
        "        assert self.car is not None\n",
        "        true_speed = np.sqrt(\n",
        "            np.square(self.car.hull.linearVelocity[0])\n",
        "            + np.square(self.car.hull.linearVelocity[1])\n",
        "        )\n",
        "\n",
        "        # simple wrapper to render if the indicator value is above a threshold\n",
        "        def render_if_min(value, points, color):\n",
        "            if abs(value) > 1e-4:\n",
        "                pygame.draw.polygon(self.surf, points=points, color=color)\n",
        "\n",
        "        render_if_min(true_speed, vertical_ind(5, 0.02 * true_speed), (255, 255, 255))\n",
        "        # ABS sensors\n",
        "        render_if_min(\n",
        "            self.car.wheels[0].omega,\n",
        "            vertical_ind(7, 0.01 * self.car.wheels[0].omega),\n",
        "            (0, 0, 255),\n",
        "        )\n",
        "        render_if_min(\n",
        "            self.car.wheels[1].omega,\n",
        "            vertical_ind(8, 0.01 * self.car.wheels[1].omega),\n",
        "            (0, 0, 255),\n",
        "        )\n",
        "        render_if_min(\n",
        "            self.car.wheels[2].omega,\n",
        "            vertical_ind(9, 0.01 * self.car.wheels[2].omega),\n",
        "            (51, 0, 255),\n",
        "        )\n",
        "        render_if_min(\n",
        "            self.car.wheels[3].omega,\n",
        "            vertical_ind(10, 0.01 * self.car.wheels[3].omega),\n",
        "            (51, 0, 255),\n",
        "        )\n",
        "\n",
        "        render_if_min(\n",
        "            self.car.wheels[0].joint.angle,\n",
        "            horiz_ind(20, -10.0 * self.car.wheels[0].joint.angle),\n",
        "            (0, 255, 0),\n",
        "        )\n",
        "        render_if_min(\n",
        "            self.car.hull.angularVelocity,\n",
        "            horiz_ind(30, -0.8 * self.car.hull.angularVelocity),\n",
        "            (255, 0, 0),\n",
        "        )\n",
        "\n",
        "    def _draw_colored_polygon(\n",
        "        self, surface, poly, color, zoom, translation, angle, clip=True\n",
        "    ):\n",
        "        poly = [pygame.math.Vector2(c).rotate_rad(angle) for c in poly]\n",
        "        poly = [\n",
        "            (c[0] * zoom + translation[0], c[1] * zoom + translation[1]) for c in poly\n",
        "        ]\n",
        "        # This checks if the polygon is out of bounds of the screen, and we skip drawing if so.\n",
        "        # Instead of calculating exactly if the polygon and screen overlap,\n",
        "        # we simply check if the polygon is in a larger bounding box whose dimension\n",
        "        # is greater than the screen by MAX_SHAPE_DIM, which is the maximum\n",
        "        # diagonal length of an environment object\n",
        "        if not clip or any(\n",
        "            (-MAX_SHAPE_DIM <= coord[0] <= WINDOW_W + MAX_SHAPE_DIM)\n",
        "            and (-MAX_SHAPE_DIM <= coord[1] <= WINDOW_H + MAX_SHAPE_DIM)\n",
        "            for coord in poly\n",
        "        ):\n",
        "            gfxdraw.aapolygon(self.surf, poly, color)\n",
        "            gfxdraw.filled_polygon(self.surf, poly, color)\n",
        "\n",
        "    def _create_image_array(self, screen, size):\n",
        "        scaled_screen = pygame.transform.smoothscale(screen, size)\n",
        "        return np.transpose(\n",
        "            np.array(pygame.surfarray.pixels3d(scaled_screen)), axes=(1, 0, 2)\n",
        "        )\n",
        "\n",
        "    def close(self):\n",
        "        if self.screen is not None:\n",
        "            pygame.display.quit()\n",
        "            self.isopen = False\n",
        "            pygame.quit()\n",
        "\n"
      ],
      "metadata": {
        "id": "uqD7R97wV0mP",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Env Demo**\n",
        "#@markdown Standard Gym Env (0.26.0 API)\n",
        "#@markdown Car Racing Demo\n",
        "\n",
        "# 0. create env object\n",
        "env = CarRacing()\n",
        "\n",
        "# 2. must reset before use\n",
        "obs = env.reset()\n",
        "\n",
        "# 3. 2D positional action space [0,512]\n",
        "action = env.action_space.sample()\n",
        "\n",
        "# 4. Standard gym step method\n",
        "obs, reward, done, _ ,info = env.step(action)\n",
        "\n",
        "# prints and explains each dimension of the observation and action vectors\n",
        "with np.printoptions(precision=4, suppress=True, threshold=5):\n",
        "\n",
        "    #print(obs)\n",
        "\n",
        "    print(\"obs['image'].shape:\", obs['image'].shape, \"Box(0, 255, (96, 96, 3), uint8)\")\n",
        "    print(\"obs['velocity'].shape:\", obs['velocity'].shape , \"numpy.float64\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyUiDbc5WzpE",
        "outputId": "68d85db6-7428-4452-f357-6199717c2d73"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "obs['image'].shape: (96, 96, 3) Box(0, 255, (96, 96, 3), uint8)\n",
            "obs['velocity'].shape: () numpy.float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Dataset**\n",
        "#@markdown\n",
        "#@markdown Defines `CarRacingDataset` and helper functions\n",
        "#@markdown\n",
        "#@markdown The dataset class\n",
        "#@markdown - Load data from a zarr storage\n",
        "#@markdown - Normalizes each dimension of non-images and actions to [-1,1]\n",
        "#@markdown - Returns\n",
        "#@markdown  - All possible segments with length `pred_horizon`\n",
        "#@markdown  - Pads the beginning and the end of each episode with repetition\n",
        "#@markdown  - key `image`: shape (obs_hoirzon, 3, 96, 96)\n",
        "#@markdown  - key `velocity`: shape (obs_hoirzon, 1)\n",
        "#@markdown  - key `action`: shape (pred_horizon, 3) \n",
        "\n",
        "def create_sample_indices(\n",
        "        episode_ends:np.ndarray, sequence_length:int, \n",
        "        pad_before: int=0, pad_after: int=0):\n",
        "    indices = list()\n",
        "    for i in range(len(episode_ends)):\n",
        "        start_idx = 0\n",
        "        if i > 0:\n",
        "            start_idx = episode_ends[i-1]\n",
        "        end_idx = episode_ends[i]\n",
        "        episode_length = end_idx - start_idx\n",
        "        \n",
        "        min_start = -pad_before\n",
        "        max_start = episode_length - sequence_length + pad_after\n",
        "        \n",
        "        # range stops one idx before end\n",
        "        for idx in range(min_start, max_start+1):\n",
        "            buffer_start_idx = max(idx, 0) + start_idx\n",
        "            buffer_end_idx = min(idx+sequence_length, episode_length) + start_idx\n",
        "            start_offset = buffer_start_idx - (idx+start_idx)\n",
        "            end_offset = (idx+sequence_length+start_idx) - buffer_end_idx\n",
        "            sample_start_idx = 0 + start_offset\n",
        "            sample_end_idx = sequence_length - end_offset\n",
        "            indices.append([\n",
        "                buffer_start_idx, buffer_end_idx, \n",
        "                sample_start_idx, sample_end_idx])\n",
        "    indices = np.array(indices)\n",
        "    return indices\n",
        "\n",
        "\n",
        "def sample_sequence(train_data, sequence_length,\n",
        "                    buffer_start_idx, buffer_end_idx, \n",
        "                    sample_start_idx, sample_end_idx):\n",
        "    result = dict()\n",
        "    for key, input_arr in train_data.items():\n",
        "        sample = input_arr[buffer_start_idx:buffer_end_idx]\n",
        "        data = sample\n",
        "        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n",
        "            data = np.zeros(\n",
        "                shape=(sequence_length,) + input_arr.shape[1:],\n",
        "                dtype=input_arr.dtype)\n",
        "            if sample_start_idx > 0:\n",
        "                data[:sample_start_idx] = sample[0]\n",
        "            if sample_end_idx < sequence_length:\n",
        "                data[sample_end_idx:] = sample[-1]\n",
        "            data[sample_start_idx:sample_end_idx] = sample\n",
        "        result[key] = data\n",
        "    return result\n",
        "\n",
        "# normalize data\n",
        "def get_data_stats(data):\n",
        "    data = data.reshape(-1,data.shape[-1])\n",
        "    stats = {\n",
        "        'min': np.min(data, axis=0),\n",
        "        'max': np.max(data, axis=0)\n",
        "    }\n",
        "    return stats\n",
        "\n",
        "def normalize_data(data, stats):\n",
        "    # nomalize to [0,1]\n",
        "    ndata = (data - stats['min']) / (stats['max'] - stats['min'])\n",
        "    # normalize to [-1, 1]\n",
        "    ndata = ndata * 2 - 1\n",
        "    return ndata\n",
        "\n",
        "def unnormalize_data(ndata, stats):\n",
        "    ndata = (ndata + 1) / 2\n",
        "    data = ndata * (stats['max'] - stats['min']) + stats['min']\n",
        "    return data\n",
        "\n",
        "# dataset\n",
        "class CarRacingDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, \n",
        "                 dataset_path: str,\n",
        "                 pred_horizon: int, \n",
        "                 obs_horizon: int, \n",
        "                 action_horizon: int):\n",
        "        \n",
        "        # read from zarr dataset\n",
        "        dataset_root = zarr.open(dataset_path, 'r')\n",
        "        \n",
        "        # float32, [0,1], (N,96,96,3)\n",
        "        train_image_data = dataset_root['data']['img'][:]\n",
        "        train_image_data = np.moveaxis(train_image_data, -1,1)\n",
        "        # (N,3,96,96)\n",
        "\n",
        "        # (N, D)\n",
        "        train_data = {\n",
        "            # velocity of var\n",
        "            'car_vel': dataset_root['data']['velocity'][:], # (T,1)\n",
        "            'action': dataset_root['data']['action'][:] #(T,3)\n",
        "        }\n",
        "        episode_ends = dataset_root['meta']['episode_ends'][:]\n",
        "        \n",
        "        # compute start and end of each state-action sequence\n",
        "        # also handles padding\n",
        "        indices = create_sample_indices(\n",
        "            episode_ends=episode_ends,\n",
        "            sequence_length=pred_horizon,\n",
        "            pad_before=obs_horizon-1,\n",
        "            pad_after=action_horizon-1)\n",
        "\n",
        "        # compute statistics and normalized data to [-1,1]\n",
        "        stats = dict()\n",
        "        normalized_train_data = dict()\n",
        "        for key, data in train_data.items():\n",
        "            stats[key] = get_data_stats(data)\n",
        "            normalized_train_data[key] = normalize_data(data, stats[key])\n",
        "        \n",
        "        # images are already normalized\n",
        "        normalized_train_data['image'] = train_image_data\n",
        "\n",
        "        self.indices = indices\n",
        "        self.stats = stats\n",
        "        self.normalized_train_data = normalized_train_data\n",
        "        self.pred_horizon = pred_horizon\n",
        "        self.action_horizon = action_horizon\n",
        "        self.obs_horizon = obs_horizon\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # get the start/end indices for this datapoint\n",
        "        buffer_start_idx, buffer_end_idx, \\\n",
        "            sample_start_idx, sample_end_idx = self.indices[idx]\n",
        "\n",
        "        # get nomralized data using these indices\n",
        "        nsample = sample_sequence(\n",
        "            train_data=self.normalized_train_data,\n",
        "            sequence_length=self.pred_horizon,\n",
        "            buffer_start_idx=buffer_start_idx,\n",
        "            buffer_end_idx=buffer_end_idx,\n",
        "            sample_start_idx=sample_start_idx,\n",
        "            sample_end_idx=sample_end_idx\n",
        "        )\n",
        "\n",
        "        # discard unused observations\n",
        "        nsample['image'] = nsample['image'][:self.obs_horizon,:]\n",
        "        nsample['car_vel'] = nsample['car_vel'][:self.obs_horizon,:]\n",
        "        return nsample\n"
      ],
      "metadata": {
        "id": "PsPVkYnVXKhZ",
        "cellView": "form"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Dataset Demo**\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/ActionPrediction/data/carracing_data_singleEpisode.zarr.zip'\n",
        "# download demonstration data from Google Drive if not available\n",
        "if not os.path.isfile(dataset_path):\n",
        "    print(\"Downloading File from Google drive\")\n",
        "    id = \"11-lAIOTvJSvFA99f1R2EmCaCuosMH-x6\"\n",
        "    dataset_path = 'carracing_data_singleEpisode.zarr.zip'\n",
        "    gdown.download(id=id, output=dataset_path, quiet=False)\n",
        "zarr_array = zarr.open(dataset_path, mode='r')\n",
        "\n",
        "\n",
        "# parameters\n",
        "pred_horizon = 8\n",
        "obs_horizon = 3\n",
        "action_horizon = 2\n",
        "\n",
        "# create dataset from file\n",
        "dataset = CarRacingDataset(\n",
        "    dataset_path=dataset_path,\n",
        "    pred_horizon=pred_horizon,\n",
        "    obs_horizon=obs_horizon,\n",
        "    action_horizon=action_horizon\n",
        ")\n",
        "# save training data statistics (min, max) for each dim\n",
        "stats = dataset.stats\n",
        "#print(dataset.normalized_train_data.keys())\n",
        "#print(list(dataset.normalized_train_data.keys())[0], \": \", dataset.normalized_train_data[list(dataset.normalized_train_data.keys())[0]].shape)\n",
        "#print(list(dataset.normalized_train_data.keys())[1], \": \", dataset.normalized_train_data[list(dataset.normalized_train_data.keys())[1]].shape)\n",
        "#print(list(dataset.normalized_train_data.keys())[2], \": \", dataset.normalized_train_data[list(dataset.normalized_train_data.keys())[2]].shape)\n",
        "\n",
        "# create dataloader\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=128,\n",
        "    num_workers=4,\n",
        "    shuffle=True,\n",
        "    # accelerate cpu-gpu transfer\n",
        "    pin_memory=True, \n",
        "    # don't kill worker process afte each epoch\n",
        "    persistent_workers=True \n",
        ")\n",
        "\n",
        "# visualize data in batch\n",
        "batch = next(iter(dataloader))\n",
        "\n",
        "print()\n",
        "print(\"Visualizing Batch and Data structure\")\n",
        "for key, value in batch.items():\n",
        "        print(f'Key: {key}')\n",
        "        print(f'Shape: ({len(value)}, {len(value[0])})')\n",
        "        print(value.shape)\n",
        "        \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdV8ITzY6f23",
        "outputId": "f186082f-16fb-4537-e67c-dbf4b0aec2e0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualizing Batch and Data structure\n",
            "Key: car_vel\n",
            "Shape: (128, 3)\n",
            "torch.Size([128, 3, 1])\n",
            "Key: action\n",
            "Shape: (128, 8)\n",
            "torch.Size([128, 8, 3])\n",
            "Key: image\n",
            "Shape: (128, 3)\n",
            "torch.Size([128, 3, 3, 96, 96])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Vision Encoder**\n",
        "#@markdown\n",
        "#@markdown Defines helper functions:\n",
        "#@markdown - `get_resnet` to initialize standard ResNet vision encoder\n",
        "#@markdown - `replace_bn_with_gn` to replace all BatchNorm layers with GroupNorm\n",
        "\n",
        "def get_resnet(name:str, weights=None, **kwargs) -> nn.Module:\n",
        "    \"\"\"\n",
        "    name: resnet18, resnet34, resnet50\n",
        "    weights: \"IMAGENET1K_V1\", None\n",
        "    \"\"\"\n",
        "    # Use standard ResNet implementation from torchvision\n",
        "    func = getattr(torchvision.models, name)\n",
        "    resnet = func(weights=weights, **kwargs)\n",
        "\n",
        "    # remove the final fully connected layer\n",
        "    # for resnet18, the output dim should be 512\n",
        "    resnet.fc = torch.nn.Identity()\n",
        "    return resnet\n",
        "\n",
        "\n",
        "def replace_submodules(\n",
        "        root_module: nn.Module, \n",
        "        predicate: Callable[[nn.Module], bool], \n",
        "        func: Callable[[nn.Module], nn.Module]) -> nn.Module:\n",
        "    \"\"\"\n",
        "    Replace all submodules selected by the predicate with\n",
        "    the output of func.\n",
        "\n",
        "    predicate: Return true if the module is to be replaced.\n",
        "    func: Return new module to use.\n",
        "    \"\"\"\n",
        "    if predicate(root_module):\n",
        "        return func(root_module)\n",
        "\n",
        "    bn_list = [k.split('.') for k, m \n",
        "        in root_module.named_modules(remove_duplicate=True) \n",
        "        if predicate(m)]\n",
        "    for *parent, k in bn_list:\n",
        "        parent_module = root_module\n",
        "        if len(parent) > 0:\n",
        "            parent_module = root_module.get_submodule('.'.join(parent))\n",
        "        if isinstance(parent_module, nn.Sequential):\n",
        "            src_module = parent_module[int(k)]\n",
        "        else:\n",
        "            src_module = getattr(parent_module, k)\n",
        "        tgt_module = func(src_module)\n",
        "        if isinstance(parent_module, nn.Sequential):\n",
        "            parent_module[int(k)] = tgt_module\n",
        "        else:\n",
        "            setattr(parent_module, k, tgt_module)\n",
        "    # verify that all modules are replaced\n",
        "    bn_list = [k.split('.') for k, m \n",
        "        in root_module.named_modules(remove_duplicate=True) \n",
        "        if predicate(m)]\n",
        "    assert len(bn_list) == 0\n",
        "    return root_module\n",
        "\n",
        "def replace_bn_with_gn(\n",
        "    root_module: nn.Module, \n",
        "    features_per_group: int=16) -> nn.Module:\n",
        "    \"\"\"\n",
        "    Relace all BatchNorm layers with GroupNorm.\n",
        "    \"\"\"\n",
        "    replace_submodules(\n",
        "        root_module=root_module,\n",
        "        predicate=lambda x: isinstance(x, nn.BatchNorm2d),\n",
        "        func=lambda x: nn.GroupNorm(\n",
        "            num_groups=x.num_features//features_per_group, \n",
        "            num_channels=x.num_features)\n",
        "    )\n",
        "    return root_module\n"
      ],
      "metadata": {
        "id": "3ljHno7z8uyY",
        "cellView": "form"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Network**\n",
        "#@markdown\n",
        "#@markdown Defines a 1D UNet architecture `ConditionalUnet1D`\n",
        "#@markdown as the noies prediction network\n",
        "#@markdown\n",
        "#@markdown Components\n",
        "#@markdown - `SinusoidalPosEmb` Positional encoding for the diffusion iteration k\n",
        "#@markdown - `Downsample1d` Strided convolution to reduce temporal resolution\n",
        "#@markdown - `Upsample1d` Transposed convolution to increase temporal resolution\n",
        "#@markdown - `Conv1dBlock` Conv1d --> GroupNorm --> Mish\n",
        "#@markdown - `ConditionalResidualBlock1D` Takes two inputs `x` and `cond`. \\\n",
        "#@markdown `x` is passed through 2 `Conv1dBlock` stacked together with residual connection. \n",
        "#@markdown `cond` is applied to `x` with [FiLM](https://arxiv.org/abs/1709.07871) conditioning.\n",
        "\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class Downsample1d(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class Upsample1d(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class Conv1dBlock(nn.Module):\n",
        "    '''\n",
        "        Conv1d --> GroupNorm --> Mish\n",
        "    '''\n",
        "\n",
        "    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
        "            nn.GroupNorm(n_groups, out_channels),\n",
        "            nn.Mish(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class ConditionalResidualBlock1D(nn.Module):\n",
        "    def __init__(self, \n",
        "            in_channels, \n",
        "            out_channels, \n",
        "            cond_dim,\n",
        "            kernel_size=3,\n",
        "            n_groups=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n",
        "            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n",
        "        ])\n",
        "\n",
        "        # FiLM modulation https://arxiv.org/abs/1709.07871\n",
        "        # predicts per-channel scale and bias\n",
        "        cond_channels = out_channels * 2\n",
        "        self.out_channels = out_channels\n",
        "        self.cond_encoder = nn.Sequential(\n",
        "            nn.Mish(),\n",
        "            nn.Linear(cond_dim, cond_channels),\n",
        "            nn.Unflatten(-1, (-1, 1))\n",
        "        )\n",
        "\n",
        "        # make sure dimensions compatible\n",
        "        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n",
        "            if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        '''\n",
        "            x : [ batch_size x in_channels x horizon ]\n",
        "            cond : [ batch_size x cond_dim]\n",
        "\n",
        "            returns:\n",
        "            out : [ batch_size x out_channels x horizon ]\n",
        "        '''\n",
        "        out = self.blocks[0](x)\n",
        "        embed = self.cond_encoder(cond)\n",
        "\n",
        "        embed = embed.reshape(\n",
        "            embed.shape[0], 2, self.out_channels, 1)\n",
        "        scale = embed[:,0,...]\n",
        "        bias = embed[:,1,...]\n",
        "        out = scale * out + bias\n",
        "\n",
        "        out = self.blocks[1](out)\n",
        "        out = out + self.residual_conv(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ConditionalUnet1D(nn.Module):\n",
        "    def __init__(self, \n",
        "        input_dim,\n",
        "        global_cond_dim,\n",
        "        diffusion_step_embed_dim=256,\n",
        "        down_dims=[256,512,1024],\n",
        "        kernel_size=5,\n",
        "        n_groups=8\n",
        "        ):\n",
        "        \"\"\"\n",
        "        input_dim: Dim of actions.\n",
        "        global_cond_dim: Dim of global conditioning applied with FiLM \n",
        "          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n",
        "        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n",
        "        down_dims: Channel size for each UNet level. \n",
        "          The length of this array determines numebr of levels.\n",
        "        kernel_size: Conv kernel size\n",
        "        n_groups: Number of groups for GroupNorm\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        all_dims = [input_dim] + list(down_dims)\n",
        "        start_dim = down_dims[0]\n",
        "\n",
        "        dsed = diffusion_step_embed_dim\n",
        "        diffusion_step_encoder = nn.Sequential(\n",
        "            SinusoidalPosEmb(dsed),\n",
        "            nn.Linear(dsed, dsed * 4),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(dsed * 4, dsed),\n",
        "        )\n",
        "        cond_dim = dsed + global_cond_dim\n",
        "\n",
        "        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n",
        "        mid_dim = all_dims[-1]\n",
        "        self.mid_modules = nn.ModuleList([\n",
        "            ConditionalResidualBlock1D(\n",
        "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
        "                kernel_size=kernel_size, n_groups=n_groups\n",
        "            ),\n",
        "            ConditionalResidualBlock1D(\n",
        "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
        "                kernel_size=kernel_size, n_groups=n_groups\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "        down_modules = nn.ModuleList([])\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (len(in_out) - 1)\n",
        "            down_modules.append(nn.ModuleList([\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_in, dim_out, cond_dim=cond_dim, \n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_out, dim_out, cond_dim=cond_dim, \n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                Downsample1d(dim_out) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "\n",
        "        up_modules = nn.ModuleList([])\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "            is_last = ind >= (len(in_out) - 1)\n",
        "            up_modules.append(nn.ModuleList([\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_out*2, dim_in, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_in, dim_in, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                Upsample1d(dim_in) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "        \n",
        "        final_conv = nn.Sequential(\n",
        "            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n",
        "            nn.Conv1d(start_dim, input_dim, 1),\n",
        "        )\n",
        "\n",
        "        self.diffusion_step_encoder = diffusion_step_encoder\n",
        "        self.up_modules = up_modules\n",
        "        self.down_modules = down_modules\n",
        "        self.final_conv = final_conv\n",
        "\n",
        "        print(\"number of parameters: {:e}\".format(\n",
        "            sum(p.numel() for p in self.parameters()))\n",
        "        )\n",
        "\n",
        "    def forward(self, \n",
        "            sample: torch.Tensor, \n",
        "            timestep: Union[torch.Tensor, float, int], \n",
        "            global_cond=None):\n",
        "        \"\"\"\n",
        "        x: (B,T,input_dim)\n",
        "        timestep: (B,) or int, diffusion step\n",
        "        global_cond: (B,global_cond_dim)\n",
        "        output: (B,T,input_dim)\n",
        "        \"\"\"\n",
        "        # (B,T,C)\n",
        "        sample = sample.moveaxis(-1,-2)\n",
        "        # (B,C,T)\n",
        "\n",
        "        # 1. time\n",
        "        timesteps = timestep\n",
        "        if not torch.is_tensor(timesteps):\n",
        "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
        "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
        "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
        "            timesteps = timesteps[None].to(sample.device)\n",
        "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
        "        timesteps = timesteps.expand(sample.shape[0])\n",
        "\n",
        "        global_feature = self.diffusion_step_encoder(timesteps)\n",
        "\n",
        "        if global_cond is not None:\n",
        "            global_feature = torch.cat([\n",
        "                global_feature, global_cond\n",
        "            ], axis=-1)\n",
        "        \n",
        "        x = sample\n",
        "        h = []\n",
        "        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n",
        "            x = resnet(x, global_feature)\n",
        "            x = resnet2(x, global_feature)\n",
        "            h.append(x)\n",
        "            x = downsample(x)\n",
        "\n",
        "        for mid_module in self.mid_modules:\n",
        "            x = mid_module(x, global_feature)\n",
        "\n",
        "        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = resnet(x, global_feature)\n",
        "            x = resnet2(x, global_feature)\n",
        "            x = upsample(x)\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "\n",
        "        # (B,C,T)\n",
        "        x = x.moveaxis(-1,-2)\n",
        "        # (B,T,C)\n",
        "        return x\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RjIVXTd6c4PU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Network Demo**\n",
        "\n",
        "# construct ResNet18 encoder\n",
        "# if you have multiple camera views, use seperate encoder weights for each view.\n",
        "vision_encoder = get_resnet('resnet18')\n",
        "\n",
        "# IMPORTANT!\n",
        "# replace all BatchNorm with GroupNorm to work with EMA\n",
        "# performance will tank if you forget to do this!\n",
        "vision_encoder = replace_bn_with_gn(vision_encoder)\n",
        "\n",
        "\n",
        "## Dimensions of Features ## \n",
        "\n",
        "# ResNet18 has output dim of 512\n",
        "vision_feature_dim = 512\n",
        "# Car Velocity is 1 dimensional\n",
        "lowdim_obs_dim = 1\n",
        "# observation feature has 514 dims in total per step\n",
        "obs_dim = vision_feature_dim + lowdim_obs_dim\n",
        "# Action space is 3 dimensional\n",
        "action_dim = 3\n",
        "\n",
        "# create network object\n",
        "noise_pred_net = ConditionalUnet1D(\n",
        "    input_dim=action_dim,\n",
        "    global_cond_dim=obs_dim*obs_horizon\n",
        ")\n",
        "\n",
        "# the final arch has 2 parts\n",
        "nets = nn.ModuleDict({\n",
        "    'vision_encoder': vision_encoder,\n",
        "    'noise_pred_net': noise_pred_net\n",
        "})\n",
        "\n",
        "# demo\n",
        "with torch.no_grad():\n",
        "    # example inputs\n",
        "    image = torch.zeros((1, obs_horizon,3,96,96))\n",
        "    agent_vel = torch.zeros((1, obs_horizon, 1))\n",
        "    # vision encoder\n",
        "    image_features = nets['vision_encoder'](\n",
        "        image.flatten(end_dim=1))\n",
        "    # (2,512)\n",
        "    image_features = image_features.reshape(*image.shape[:2],-1)\n",
        "    # (1,2,512)\n",
        "    obs = torch.cat([image_features, agent_vel],dim=-1)\n",
        "    # (1,2,514)\n",
        "\n",
        "    noised_action = torch.randn((1, pred_horizon, action_dim))\n",
        "    diffusion_iter = torch.zeros((1,))\n",
        "\n",
        "    # the noise prediction network\n",
        "    # takes noisy action, diffusion iteration and observation as input\n",
        "    # predicts the noise added to action\n",
        "    noise = nets['noise_pred_net'](\n",
        "        sample=noised_action, \n",
        "        timestep=diffusion_iter,\n",
        "        global_cond=obs.flatten(start_dim=1))\n",
        "\n",
        "    # illustration of removing noise \n",
        "    # the actual noise removal is performed by NoiseScheduler \n",
        "    # and is dependent on the diffusion noise schedule\n",
        "    denoised_action = noised_action - noise\n",
        "\n",
        "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
        "num_diffusion_iters = 100\n",
        "noise_scheduler = DDPMScheduler(\n",
        "    num_train_timesteps=num_diffusion_iters,\n",
        "    # the choise of beta schedule has big impact on performance\n",
        "    # we found squared cosine works the best\n",
        "    beta_schedule='squaredcos_cap_v2',\n",
        "    # clip output to [-1,1] to improve stability\n",
        "    clip_sample=True,\n",
        "    # our network predicts noise (instead of denoised action)\n",
        "    prediction_type='epsilon'\n",
        ")\n",
        "\n",
        "# device transfer\n",
        "device = torch.device('cuda')\n",
        "_ = nets.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO4xvJ__dAdv",
        "outputId": "c11f4b24-298f-4a20-a70f-614bea3fd750"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 8.727476e+07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Training**\n",
        "#@markdown\n",
        "#@markdown Takes about 2.5 hours. If you don't want to wait, skip to the next cell\n",
        "#@markdown to load pre-trained weights\n",
        "\n",
        "num_epochs = 100\n",
        "# Build training checkpoints for reloading\n",
        "# If used make sure you have the desired checkpoints available:\n",
        "createCheckpoints = True\n",
        "if createCheckpoints:\n",
        "  checkpoint_dir = \"/content/drive/MyDrive/ActionPrediction/checkpoints\"\n",
        "  checkpoint_counter = 0\n",
        "  checkpoint_frequency = 10  # save model every 10 epochs\n",
        "\n",
        "min_loss = float('inf')\n",
        "# Exponential Moving Average\n",
        "# accelerates training and improves stability\n",
        "# holds a copy of the model weights\n",
        "ema = EMAModel(\n",
        "    model=nets,\n",
        "    power=0.75)\n",
        "\n",
        "# Standard ADAM optimizer\n",
        "# Note that EMA parametesr are not optimized\n",
        "optimizer = torch.optim.AdamW(\n",
        "    params=nets.parameters(), \n",
        "    lr=1e-4, weight_decay=1e-6)\n",
        "\n",
        "# Cosine LR schedule with linear warmup\n",
        "lr_scheduler = get_scheduler(\n",
        "    name='cosine',\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=500,\n",
        "    num_training_steps=len(dataloader) * num_epochs\n",
        ")\n",
        "\n",
        "\n",
        "with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
        "    # epoch loop\n",
        "    for epoch_idx in tglobal:\n",
        "        epoch_loss = list()\n",
        "        # batch loop\n",
        "        with tqdm(dataloader, desc='Batch', leave=False) as tepoch:\n",
        "            for nbatch in tepoch:\n",
        "                # data normalized in dataset\n",
        "                # device transfer\n",
        "                nimage = nbatch['image'][:,:obs_horizon].to(device)\n",
        "                ncar_vel = nbatch['car_vel'][:,:obs_horizon].to(device)\n",
        "                naction = nbatch['action'].to(device)\n",
        "                B = ncar_vel.shape[0]\n",
        "\n",
        "                # encoder vision features\n",
        "                image_features = nets['vision_encoder'](\n",
        "                    nimage.flatten(end_dim=1))\n",
        "                image_features = image_features.reshape(\n",
        "                    *nimage.shape[:2],-1)\n",
        "                # (B,obs_horizon,D)\n",
        "\n",
        "                # concatenate vision feature and low-dim obs\n",
        "                obs_features = torch.cat([image_features, ncar_vel], dim=-1)\n",
        "                obs_cond = obs_features.flatten(start_dim=1)\n",
        "                # (B, obs_horizon * obs_dim)\n",
        "\n",
        "                # sample noise to add to actions\n",
        "                noise = torch.randn(naction.shape, device=device)\n",
        "\n",
        "                # sample a diffusion iteration for each data point\n",
        "                timesteps = torch.randint(\n",
        "                    0, noise_scheduler.config.num_train_timesteps, \n",
        "                    (B,), device=device\n",
        "                ).long()\n",
        "\n",
        "                # add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
        "                # (this is the forward diffusion process)\n",
        "                noisy_actions = noise_scheduler.add_noise(\n",
        "                    naction, noise, timesteps)\n",
        "                \n",
        "#                print(naction.shape)\n",
        "#                print(noisy_actions.shape)\n",
        "                \n",
        "                # predict the noise residual\n",
        "                noise_pred = noise_pred_net(\n",
        "                    noisy_actions, timesteps, global_cond=obs_cond)\n",
        "                \n",
        "                # L2 loss\n",
        "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
        "\n",
        "                # optimize\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                # step lr scheduler every batch\n",
        "                # this is different from standard pytorch behavior\n",
        "                lr_scheduler.step()\n",
        "\n",
        "                # update Exponential Moving Average of the model weights\n",
        "                ema.step(nets)\n",
        "\n",
        "                # logging\n",
        "                loss_cpu = loss.item()\n",
        "                epoch_loss.append(loss_cpu)\n",
        "                tepoch.set_postfix(loss=loss_cpu)\n",
        "        tglobal.set_postfix(loss=np.mean(epoch_loss))\n",
        "\n",
        "        if epoch_idx > 0 and epoch_idx % checkpoint_frequency == 0 and createCheckpoints==True:\n",
        "            # compute mean epoch loss\n",
        "            mean_epoch_loss = np.mean(epoch_loss)\n",
        "            # save model if mean epoch loss is smaller than the loss after the previous checkpoint\n",
        "            if mean_epoch_loss < min_loss:\n",
        "                min_loss = mean_epoch_loss\n",
        "                checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_Epoch{epoch_idx}.pt\")\n",
        "                torch.save(ema.averaged_model.state_dict(), checkpoint_path)\n",
        "                checkpoint_counter += 1\n",
        "                print(f\"Model saved at epoch {epoch_idx}\")\n",
        "\n",
        "\n",
        "# Weights of the EMA model\n",
        "# is used for inference\n",
        "ema_net = ema.averaged_model\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444,
          "referenced_widgets": [
            "1d69b5d787a3484290407ce68ac7b765",
            "a28568446d954460bd461496131e6b2d",
            "b6f53460b7594f2488a30beecadab190",
            "4745ce986c2941d8b2fd6d543211ddc2",
            "3592ad22550f4bb8ab78f6c4935475c8",
            "f729f98087c74e1e94b9f6ac1d8629d6",
            "069b4d1bd9c84f62b4b0d111ae4374ff",
            "77dcbbb0c73b4647a064720d38659eb0",
            "92f8d292b0d84062bea3eea0e64e3b27",
            "8c3e11ab000f47cc9c2bd4336b20b1fc",
            "d4216db049b144689959de94ab022844",
            "d210f1c104c54f9094e23ab5d601447c",
            "7999fb8194d64130a1c9ac5249daa9af",
            "120b19d92c6e41c39824822f00887f89",
            "ac13ad050f9f499cbeaa01f0d12a5da3",
            "02172c34e9fc46cfa13834c948aed9b0",
            "b2cbef52354248f287a150b057821605",
            "aa2a16bf98514a1bb4e7b49a1b91ba7e",
            "47571b864cd24dae94d31f4879dbdbc5",
            "347093edb5ff4c0a8338d1fa65320e64",
            "24cb2a612c8642839467f6da05e28ae9",
            "da187735d7f942e4b53f724b55964638"
          ]
        },
        "id": "GGzBZc5xdwIK",
        "outputId": "d2babb8b-32ec-4295-bc2c-1a2dfc216326"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d69b5d787a3484290407ce68ac7b765"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batch:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d210f1c104c54f9094e23ab5d601447c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-974682a054c7>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;31m# encoder vision features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 image_features = nets['vision_encoder'](\n\u001b[0m\u001b[1;32m     54\u001b[0m                     nimage.flatten(end_dim=1))\n\u001b[1;32m     55\u001b[0m                 image_features = image_features.reshape(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# See note [TorchScript super()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Loading Pretrained Checkpoint**\n",
        "#@markdown Set `load_pretrained = True` to load pretrained weights.\n",
        "\n",
        "load_pretrained = False\n",
        "checkpoint_num = 90\n",
        "ema_net = nets\n",
        "\n",
        "if load_pretrained:\n",
        "  ckpt_path = f\"/content/drive/MyDrive/checkpoints/checkpoint_Epoch{checkpoint_num}.pt\"\n",
        "\n",
        "  state_dict = torch.load(ckpt_path, map_location='cuda')\n",
        "  ema_net.load_state_dict(state_dict)\n",
        "  print('Pretrained weights loaded.')\n",
        "else:\n",
        "  print(\"Skipped pretrained weight loading.\")"
      ],
      "metadata": {
        "id": "cY0q8DZLfLI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Inference**\n",
        "\n",
        "# limit enviornment interaction to 200 steps before termination\n",
        "max_steps = 1000\n",
        "env = CarRacing(render_mode='state_pixels')\n",
        "\n",
        "# get first observation\n",
        "observation = env.reset()\n",
        "observation = observation[0]\n",
        "\n",
        "# keep a queue lenght of observations horizon\n",
        "obs_deque = collections.deque(\n",
        "    [observation] * obs_horizon, maxlen=obs_horizon)\n",
        "\n",
        "# save visualization and rewards\n",
        "imgs = [env.render()]\n",
        "rewards = list()\n",
        "done = False\n",
        "step_idx = 0\n",
        "\n",
        "\n",
        "with tqdm(total=max_steps, desc=\"Eval CarRacingImageEnv\") as pbar:\n",
        "    while not done:\n",
        "        B = 1\n",
        "        # stack the last obs_horizon number of observations\n",
        "        images = np.stack(x['image'] for x in obs_deque)\n",
        "        images = np.moveaxis(images, -1,1)\n",
        "        car_vel =  np.stack(x['velocity'] for x in obs_deque)\n",
        "        # normalize observation\n",
        "        ncar_vels = normalize_data(car_vel, stats=stats['car_vel'])\n",
        "        # images are already normalized to [0,1]\n",
        "        nimages = images\n",
        "\n",
        "        # device transfer\n",
        "        nimages = torch.from_numpy(nimages).to(device, dtype=torch.float32)\n",
        "        # (2,3,96,96)\n",
        "        ncar_vels = torch.from_numpy(ncar_vels).to(device, dtype=torch.float32)\n",
        "        # (2,1)\n",
        "        ncar_vels = ncar_vels.unsqueeze(-1)\n",
        "\n",
        "\n",
        "        # infer action\n",
        "        with torch.no_grad():\n",
        "            # get image features\n",
        "            image_features = ema_net['vision_encoder'](nimages)\n",
        "            # (2,512)\n",
        "            # concat with low-dim observations\n",
        "            obs_features = torch.cat([image_features, ncar_vels], dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "            # reshape observation to (B,obs_horizon*obs_dim)\n",
        "            obs_cond = obs_features.unsqueeze(0).flatten(start_dim=1)\n",
        "\n",
        "            # initialize action from Guassian noise\n",
        "            noisy_action = torch.randn(\n",
        "                (B, pred_horizon, action_dim), device=device)\n",
        "            naction = noisy_action\n",
        "            \n",
        "            #print(noisy_action.shape)\n",
        "            \n",
        "\n",
        "            # init scheduler\n",
        "            noise_scheduler.set_timesteps(num_diffusion_iters)\n",
        "\n",
        "            for k in noise_scheduler.timesteps:\n",
        "                # predict noise\n",
        "                noise_pred = ema_net['noise_pred_net'](\n",
        "                    sample=naction, \n",
        "                    timestep=k,\n",
        "                    global_cond=obs_cond\n",
        "                )\n",
        "\n",
        "                # inverse diffusion step (remove noise)\n",
        "                naction = noise_scheduler.step(\n",
        "                    model_output=noise_pred,\n",
        "                    timestep=k,\n",
        "                    sample=naction\n",
        "                ).prev_sample\n",
        "\n",
        "        # unnormalize action\n",
        "        naction = naction.detach().to('cpu').numpy()\n",
        "        # (B, pred_horizon, action_dim)\n",
        "        naction = naction[0]\n",
        "        action_pred = unnormalize_data(naction, stats=stats['action'])\n",
        "\n",
        "        # only take action_horizon number of actions\n",
        "        start = obs_horizon - 1\n",
        "        end = start + action_horizon\n",
        "        action = action_pred[start:end,:]\n",
        "        # (action_horizon, action_dim)\n",
        "\n",
        "        # execute action_horizon number of steps\n",
        "        # without replanning\n",
        "        for i in range(len(action)):\n",
        "            # stepping env\n",
        "            obs, reward, done, _ , info = env.step(action[i])\n",
        "            # save observations\n",
        "            obs_deque.append(obs)\n",
        "            # and reward/vis\n",
        "            rewards.append(reward)\n",
        "            imgs.append(env.render())\n",
        "\n",
        "            # update progress bar\n",
        "            step_idx += 1\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix(reward=reward)\n",
        "            if step_idx > max_steps:\n",
        "                done = True\n",
        "            if done:\n",
        "                break\n"
      ],
      "metadata": {
        "id": "_9y3ihZVeIrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Visualize**\n",
        "# visualize\n",
        "from IPython.display import Video\n",
        "vwrite('vis.mp4', imgs)\n",
        "Video('vis.mp4', embed=True, width=256, height=256)"
      ],
      "metadata": {
        "id": "xl38a-ubeU5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K03bKQxi0Tr0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}